{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jinyoungkim0214/pytorch-highlevel-apis/blob/main/PyTorch_high_level_apis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKVeLItnS5uV"
      },
      "source": [
        "Copyright (C) 2020 Software Platform Lab, Seoul National University\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "\n",
        "you may not use this file except in compliance with the License.\n",
        "\n",
        "You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "\n",
        "\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "\n",
        "\n",
        "See the License for the specific language governing permissions and\n",
        "\n",
        "\n",
        "limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LQHVwI1g_PF"
      },
      "source": [
        "### Preparing MNIST Dataset\n",
        "\n",
        "Pytorch is providing a MNIST Dataset class, so we can simply import it from `torchvision.datasets` without handcrafting it. Before instantiating Dataset objects, let's first define transforms for MNIST dataset. `torchvision.transforms` module contains multiple predefined transforms. Here, we apply `ToTensor()` that converts the data into the PyTorch Tensor type and `Normalize(mean, std)` that normalizes the sample data to have given mean and standard deviation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clLlHNHGz36J"
      },
      "outputs": [],
      "source": [
        "import torchvision.transforms as transforms\n",
        "#컴퓨터비전을 위한 라이브러리\n",
        "#웬만한건 다 있음\n",
        "\n",
        "# Normalize data with mean=0.5, std=1.0\n",
        "mnist_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (1.0,))\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOUK49KAz36N"
      },
      "source": [
        "We use `torchvision.datasets.MNIST` API to instantiate MNIST Dataset objects. Having `download=True` flag, MNIST dataset will be automatically downloaded at `download_root` unless it already exists."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LydTwAzMhHw0",
        "outputId": "a34de3d0-40b4-4887-d836-edf24ee47364"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:01<00:00, 5.10MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 135kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.27MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 8.00MB/s]\n"
          ]
        }
      ],
      "source": [
        "from torchvision.datasets import MNIST\n",
        "\n",
        "# download path\n",
        "download_root = './MNIST_DATASET'\n",
        "\n",
        "train_dataset = MNIST(download_root, transform=mnist_transform, train=True, download=True)\n",
        "test_dataset = MNIST(download_root, transform=mnist_transform, train=False, download=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jE6MBKTYz36R"
      },
      "source": [
        "Finally, we instantiate DataLoader that can shuffle and batch the MNIST Dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_E6ELImz36S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8d80b70-3024-4a0e-deed-1974c3573e03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 1, 28, 28]) torch.Size([64])\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset,\n",
        "                         batch_size=BATCH_SIZE,\n",
        "                         shuffle=True)\n",
        "\n",
        "test_loader = DataLoader(dataset=test_dataset,\n",
        "                         batch_size=BATCH_SIZE,\n",
        "                         shuffle=True)\n",
        "\n",
        "imgs, labels = next(iter(train_loader))\n",
        "print(imgs.shape, labels.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QACEGZ2KAR7m"
      },
      "source": [
        "## Custom Neural Network Models\n",
        "\n",
        "PyTorch `torch.nn.Module` allows you to easily make your own custom neural network model. All you need to do is to 1) make a class that inherits `torch.nn.Module` class and 2) define `forward` method. Let's build a simple CNN model using `torch.nn` APIs.\n",
        "\n",
        "* `torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')`\n",
        "* `torch.nn.Linear(in_features, out_features, bias=True)`\n",
        "* `torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)`\n",
        "* `torch.nn.functional.relu(input, inplace=False)`\n",
        "* `torch.nn.functional.softmax(input, dim=None)`\n",
        "\n",
        "You can refer to the following links for more detailed descriptions of `torch.nn` APIs.\n",
        "* https://pytorch.org/docs/stable/nn.html\n",
        "* https://pytorch.org/docs/stable/nn.functional.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izGiUAELAR7n"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn #neuralnetwork_신경망미리구현\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.conv1= nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=2) #커널사이즈가 커지면 파라미터가 증가한다, 3*3 쓰면 파라미터가 줄기 때문에 레이어를 깊게 쌓기 가능\n",
        "        self.pool1 = nn.MaxPool2d(2) #정보손실(downsampling):채널을 늘려줘야 함, 각 채널에 저장\n",
        "\n",
        "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=2)\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(16,32, kernel_size=3, stride=1, padding=1) #2dconv바로가져올수있음 #3*3이니까padding=1일때output사이즈유지\n",
        "        self.pool3 = nn.MaxPool2d(2)\n",
        "\n",
        "        #self.conv4 = nn.Conv2d(32,64, 5, 1)\n",
        "        #self.pool4 = nn.MaxPool2d(2)\n",
        "\n",
        "        self.fc1 = nn.Linear(288, 128)\n",
        "        self.dropout = nn.Dropout(0.3) #overfitting방지\n",
        "        self.fc2 = nn.Linear(128,64)\n",
        "        self.fc3 = nn.Linear(64,10)\n",
        "\n",
        "        self.bn1 = nn.BatchNorm2d(6)\n",
        "        self.bn2 = nn.BatchNorm2d(16)\n",
        "        self.bn3 = nn.BatchNorm2d(32)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #input size ; 1*28*28\n",
        "        # First convolution layer\n",
        "        x = self.conv1(x)\n",
        "        #size:6*28*28\n",
        "        #batch normalization\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x) #max{x,0}이미구현\n",
        "        x = self.pool1(x)\n",
        "        #6*14*14\n",
        "\n",
        "\n",
        "        # Second convolution layer\n",
        "        x = self.conv2(x)\n",
        "        #16*14*14\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool2(x)\n",
        "        #16*7*7\n",
        "\n",
        "        # Third convolution layer\n",
        "        x = self.conv3(x)\n",
        "        #32*7*7\n",
        "        x = self.bn3(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool3(x)\n",
        "        #32*3*3 (3.5->3)\n",
        "\n",
        "        # (N, 256) flatten과 동일한 효과\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x #loss를crossentropy로쓰니까\n",
        "\n",
        "      #  return F.soft(x, dim=1) #아웃풋은확률_다더해서1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OJowVblz36Z"
      },
      "source": [
        "Instantiate the custom neural network model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQSRf32Zz36Z",
        "outputId": "0aa6c8f8-2150-4a9e-a944-5649cafa4f84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=288, out_features=128, bias=True)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
            "  (bn1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "net = Net()\n",
        "print(net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FenyD3rz36d"
      },
      "source": [
        "Define the loss function (`criterion`) and the optimization method (`optimizer`). In this example, cross entropy loss is used as the criterion and SGD is used as the optimizer. By having `net.parameters()` as the input for the optimizer, we are trying to apply SGD to all the trainable parameters that consist of our custom neural network model `net`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8b4ed4Xz36e"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
        "# 학습을 하려는 파라미터 :net.parameters()\n",
        "# 특정 파라미터만 학습하고자하면 변경 필요\n",
        "# 학습률 : 0.01(alpha)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nd6Js7Nz36h"
      },
      "source": [
        "Finally, connecting altoghether, we can train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jiGgTcMrz36i",
        "outputId": "2414c8ab-edfd-449b-d609-d019ab4a9743"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch: 1] train loss: 0.8558, test acc: 94.26\n",
            "[Epoch: 2] train loss: 0.1558, test acc: 97.03\n",
            "[Epoch: 3] train loss: 0.1022, test acc: 97.63\n",
            "[Epoch: 4] train loss: 0.0816, test acc: 97.68\n",
            "[Epoch: 5] train loss: 0.0700, test acc: 98.09\n",
            "[Epoch: 6] train loss: 0.0605, test acc: 98.16\n",
            "[Epoch: 7] train loss: 0.0550, test acc: 98.43\n",
            "[Epoch: 8] train loss: 0.0482, test acc: 98.23\n",
            "[Epoch: 9] train loss: 0.0445, test acc: 98.60\n",
            "[Epoch: 10] train loss: 0.0410, test acc: 98.43\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = 0.0\n",
        "    # Iteration over the train dataset\n",
        "    for i, data in enumerate(train_loader):\n",
        "        x, label = data\n",
        "        # 1. Initalize gradient values\n",
        "        optimizer.zero_grad() # 해야하는 이유 :\n",
        "\n",
        "        # 2. Forward propagation\n",
        "        model_output = net(x)\n",
        "\n",
        "        # 3. Calculate loss using the criterion\n",
        "        loss = criterion(model_output, label)  # 계산 시 위에서 설정한 크로스엔트로피 사용\n",
        "\n",
        "        # 4. Back propagation\n",
        "        loss.backward()\n",
        "\n",
        "        # 5. Weight update\n",
        "        optimizer.step()   # weight 업데이트 일어남\n",
        "\n",
        "        train_loss += loss.item()  # 각 loss를 저장\n",
        "\n",
        "\n",
        "    # Print train loss and test accuracy at the end of every epoch\n",
        "    with torch.no_grad(): # do not forget this\n",
        "        corr_num = 0\n",
        "        total_num = 0\n",
        "        # Iteration overt the test dataset to evaluate the test accuracy\n",
        "        for _, test in enumerate(test_loader):\n",
        "            test_x, test_label = test\n",
        "            test_output = net(test_x)\n",
        "            pred_label = test_output.argmax(dim=1)\n",
        "            corr = test_label[test_label == pred_label].size(0)\n",
        "            corr_num += corr\n",
        "            total_num += test_label.size(0)\n",
        "    print(\"[Epoch: %d] train loss: %.4f, test acc: %.2f\" \\\n",
        "        % (epoch + 1, train_loss / len(train_loader), corr_num / total_num * 100))\n",
        "    train_loss = 0.0\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Nubb_au4ZN78"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}